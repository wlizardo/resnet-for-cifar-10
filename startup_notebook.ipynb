{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T07:01:23.114959Z",
     "start_time": "2024-04-11T07:01:23.062920Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "print(f'PyTorch version: {torch.__version__}')\n",
    "print('*'*10)\n",
    "print(f'_CUDA version: ')\n",
    "!nvcc --version\n",
    "print('*'*10)\n",
    "print(f'CUDNN version: {torch.backends.cudnn.version()}')\n",
    "print(f'Available GPU devices: {torch.cuda.device_count()}')\n",
    "print(f'Device Name: {torch.cuda.get_device_name()}')"
   ],
   "id": "946f84020d531044",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.2.2\n",
      "**********\n",
      "_CUDA version: \n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2024 NVIDIA Corporation\n",
      "Built on Tue_Feb_27_16:28:36_Pacific_Standard_Time_2024\n",
      "Cuda compilation tools, release 12.4, V12.4.99\n",
      "Build cuda_12.4.r12.4/compiler.33961263_0\n",
      "**********\n",
      "CUDNN version: 8801\n",
      "Available GPU devices: 1\n",
      "Device Name: NVIDIA GeForce RTX 3080\n"
     ]
    }
   ],
   "execution_count": 223
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ],
   "metadata": {
    "id": "FnaBqT3xSE3z",
    "ExecuteTime": {
     "end_time": "2024-04-11T07:01:23.762248Z",
     "start_time": "2024-04-11T07:01:23.755249Z"
    }
   },
   "id": "FnaBqT3xSE3z",
   "outputs": [],
   "execution_count": 224
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Load the complete CIFAR-10 dataset from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz"
   ],
   "metadata": {
    "id": "PaURG9isSMdA"
   },
   "id": "PaURG9isSMdA"
  },
  {
   "cell_type": "code",
   "source": [
    "transform_traindata = transforms.Compose([ transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(), transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)), ])\n",
    "\n",
    "transform_testdata = transforms.Compose([ transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)), ])"
   ],
   "metadata": {
    "id": "ufnC7DtQSPVb",
    "ExecuteTime": {
     "end_time": "2024-04-11T07:01:25.064374Z",
     "start_time": "2024-04-11T07:01:25.045302Z"
    }
   },
   "id": "ufnC7DtQSPVb",
   "outputs": [],
   "execution_count": 225
  },
  {
   "cell_type": "code",
   "source": [
    "# Loading the Complete CIFAR-10 dataset\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_traindata)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_testdata)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=100, shuffle=False, num_workers=2)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GFaBDSwgST_3",
    "outputId": "0164262b-15c5-49dd-c2f4-3d2882e76575",
    "ExecuteTime": {
     "end_time": "2024-04-11T07:01:26.798781Z",
     "start_time": "2024-04-11T07:01:25.724743Z"
    }
   },
   "id": "GFaBDSwgST_3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "execution_count": 226
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Create a Model to establish a BasicBlock of Resnet"
   ],
   "metadata": {
    "id": "k4bowr45SYIY"
   },
   "id": "k4bowr45SYIY"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T07:01:28.431557Z",
     "start_time": "2024-04-11T07:01:28.411551Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 16\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n",
    "        self.linear = nn.Linear(64, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = F.avg_pool2d(out, 8)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [5,5,5])\n",
    "\n",
    "# Check number of parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Create the model\n",
    "model = ResNet18()"
   ],
   "id": "a9d62778b85fe6d4",
   "outputs": [],
   "execution_count": 227
  },
  {
   "cell_type": "code",
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "#optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)\n",
    "\n",
    "# Total Parameters of the Basic Resnet Model\n",
    "num_params = sum(p.numel() for p in model.parameters() )\n",
    "print(f\"Number of parameters from ResnetBasicBlock model is :{num_params}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VkR5iSkMSi5l",
    "outputId": "0382cd42-e770-405d-e685-979ef70b4e2b",
    "ExecuteTime": {
     "end_time": "2024-04-11T07:01:30.519302Z",
     "start_time": "2024-04-11T07:01:30.509492Z"
    }
   },
   "id": "VkR5iSkMSi5l",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters from ResnetBasicBlock model is :466906\n"
     ]
    }
   ],
   "execution_count": 228
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Now, Create a ModifiedResnet Model by changing the parameter values"
   ],
   "metadata": {
    "id": "UhR75pvWSlce"
   },
   "id": "UhR75pvWSlce"
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "u2fqjhTISmik",
    "ExecuteTime": {
     "end_time": "2024-04-11T07:01:35.220628Z",
     "start_time": "2024-04-11T07:01:35.203111Z"
    }
   },
   "id": "u2fqjhTISmik",
   "outputs": [],
   "execution_count": 228
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Train the Model"
   ],
   "metadata": {
    "id": "q-RrQu02So3n"
   },
   "id": "q-RrQu02So3n"
  },
  {
   "cell_type": "code",
   "source": [
    "def train_model(model, train_dataloader, criterion, optimizer, epochs=10):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_dataloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % 100 == 99:    # print every 100 mini-batches\n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                      (epoch + 1, i + 1, running_loss / 100))\n",
    "                running_loss = 0.0\n",
    "\n",
    "    print('Finished Training')"
   ],
   "metadata": {
    "id": "Kt8pkyiVSpwn",
    "ExecuteTime": {
     "end_time": "2024-04-11T07:01:38.485962Z",
     "start_time": "2024-04-11T07:01:38.470445Z"
    }
   },
   "id": "Kt8pkyiVSpwn",
   "outputs": [],
   "execution_count": 229
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T07:12:50.870884Z",
     "start_time": "2024-04-11T07:01:42.318711Z"
    }
   },
   "cell_type": "code",
   "source": "train_model(model, train_dataloader, criterion, optimizer, epochs=50)",
   "id": "41be515f0cecd9f4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 1.821\n",
      "[1,   200] loss: 1.523\n",
      "[1,   300] loss: 1.383\n",
      "[2,   100] loss: 1.152\n",
      "[2,   200] loss: 1.055\n",
      "[2,   300] loss: 1.019\n",
      "[3,   100] loss: 0.919\n",
      "[3,   200] loss: 0.890\n",
      "[3,   300] loss: 0.846\n",
      "[4,   100] loss: 0.770\n",
      "[4,   200] loss: 0.769\n",
      "[4,   300] loss: 0.752\n",
      "[5,   100] loss: 0.688\n",
      "[5,   200] loss: 0.680\n",
      "[5,   300] loss: 0.657\n",
      "[6,   100] loss: 0.609\n",
      "[6,   200] loss: 0.634\n",
      "[6,   300] loss: 0.615\n",
      "[7,   100] loss: 0.571\n",
      "[7,   200] loss: 0.569\n",
      "[7,   300] loss: 0.560\n",
      "[8,   100] loss: 0.531\n",
      "[8,   200] loss: 0.542\n",
      "[8,   300] loss: 0.526\n",
      "[9,   100] loss: 0.505\n",
      "[9,   200] loss: 0.500\n",
      "[9,   300] loss: 0.503\n",
      "[10,   100] loss: 0.469\n",
      "[10,   200] loss: 0.467\n",
      "[10,   300] loss: 0.479\n",
      "[11,   100] loss: 0.453\n",
      "[11,   200] loss: 0.443\n",
      "[11,   300] loss: 0.434\n",
      "[12,   100] loss: 0.402\n",
      "[12,   200] loss: 0.436\n",
      "[12,   300] loss: 0.438\n",
      "[13,   100] loss: 0.416\n",
      "[13,   200] loss: 0.414\n",
      "[13,   300] loss: 0.415\n",
      "[14,   100] loss: 0.375\n",
      "[14,   200] loss: 0.400\n",
      "[14,   300] loss: 0.392\n",
      "[15,   100] loss: 0.368\n",
      "[15,   200] loss: 0.378\n",
      "[15,   300] loss: 0.380\n",
      "[16,   100] loss: 0.344\n",
      "[16,   200] loss: 0.371\n",
      "[16,   300] loss: 0.365\n",
      "[17,   100] loss: 0.344\n",
      "[17,   200] loss: 0.344\n",
      "[17,   300] loss: 0.359\n",
      "[18,   100] loss: 0.330\n",
      "[18,   200] loss: 0.341\n",
      "[18,   300] loss: 0.332\n",
      "[19,   100] loss: 0.326\n",
      "[19,   200] loss: 0.318\n",
      "[19,   300] loss: 0.330\n",
      "[20,   100] loss: 0.301\n",
      "[20,   200] loss: 0.323\n",
      "[20,   300] loss: 0.316\n",
      "[21,   100] loss: 0.300\n",
      "[21,   200] loss: 0.300\n",
      "[21,   300] loss: 0.305\n",
      "[22,   100] loss: 0.292\n",
      "[22,   200] loss: 0.286\n",
      "[22,   300] loss: 0.304\n",
      "[23,   100] loss: 0.272\n",
      "[23,   200] loss: 0.278\n",
      "[23,   300] loss: 0.303\n",
      "[24,   100] loss: 0.258\n",
      "[24,   200] loss: 0.281\n",
      "[24,   300] loss: 0.291\n",
      "[25,   100] loss: 0.258\n",
      "[25,   200] loss: 0.274\n",
      "[25,   300] loss: 0.269\n",
      "[26,   100] loss: 0.263\n",
      "[26,   200] loss: 0.261\n",
      "[26,   300] loss: 0.261\n",
      "[27,   100] loss: 0.243\n",
      "[27,   200] loss: 0.252\n",
      "[27,   300] loss: 0.269\n",
      "[28,   100] loss: 0.246\n",
      "[28,   200] loss: 0.246\n",
      "[28,   300] loss: 0.251\n",
      "[29,   100] loss: 0.230\n",
      "[29,   200] loss: 0.238\n",
      "[29,   300] loss: 0.241\n",
      "[30,   100] loss: 0.222\n",
      "[30,   200] loss: 0.233\n",
      "[30,   300] loss: 0.238\n",
      "[31,   100] loss: 0.217\n",
      "[31,   200] loss: 0.236\n",
      "[31,   300] loss: 0.222\n",
      "[32,   100] loss: 0.212\n",
      "[32,   200] loss: 0.223\n",
      "[32,   300] loss: 0.234\n",
      "[33,   100] loss: 0.209\n",
      "[33,   200] loss: 0.215\n",
      "[33,   300] loss: 0.214\n",
      "[34,   100] loss: 0.205\n",
      "[34,   200] loss: 0.209\n",
      "[34,   300] loss: 0.219\n",
      "[35,   100] loss: 0.195\n",
      "[35,   200] loss: 0.214\n",
      "[35,   300] loss: 0.206\n",
      "[36,   100] loss: 0.189\n",
      "[36,   200] loss: 0.199\n",
      "[36,   300] loss: 0.205\n",
      "[37,   100] loss: 0.191\n",
      "[37,   200] loss: 0.190\n",
      "[37,   300] loss: 0.192\n",
      "[38,   100] loss: 0.183\n",
      "[38,   200] loss: 0.188\n",
      "[38,   300] loss: 0.195\n",
      "[39,   100] loss: 0.171\n",
      "[39,   200] loss: 0.181\n",
      "[39,   300] loss: 0.198\n",
      "[40,   100] loss: 0.169\n",
      "[40,   200] loss: 0.184\n",
      "[40,   300] loss: 0.185\n",
      "[41,   100] loss: 0.174\n",
      "[41,   200] loss: 0.186\n",
      "[41,   300] loss: 0.175\n",
      "[42,   100] loss: 0.158\n",
      "[42,   200] loss: 0.170\n",
      "[42,   300] loss: 0.180\n",
      "[43,   100] loss: 0.159\n",
      "[43,   200] loss: 0.174\n",
      "[43,   300] loss: 0.168\n",
      "[44,   100] loss: 0.168\n",
      "[44,   200] loss: 0.150\n",
      "[44,   300] loss: 0.160\n",
      "[45,   100] loss: 0.160\n",
      "[45,   200] loss: 0.162\n",
      "[45,   300] loss: 0.172\n",
      "[46,   100] loss: 0.141\n",
      "[46,   200] loss: 0.156\n",
      "[46,   300] loss: 0.170\n",
      "[47,   100] loss: 0.153\n",
      "[47,   200] loss: 0.162\n",
      "[47,   300] loss: 0.157\n",
      "[48,   100] loss: 0.144\n",
      "[48,   200] loss: 0.150\n",
      "[48,   300] loss: 0.155\n",
      "[49,   100] loss: 0.147\n",
      "[49,   200] loss: 0.144\n",
      "[49,   300] loss: 0.147\n",
      "[50,   100] loss: 0.135\n",
      "[50,   200] loss: 0.135\n",
      "[50,   300] loss: 0.149\n",
      "Finished Training\n"
     ]
    }
   ],
   "execution_count": 230
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Evaluate the Model"
   ],
   "metadata": {
    "id": "T7BVL-MpSqS5"
   },
   "id": "T7BVL-MpSqS5"
  },
  {
   "cell_type": "code",
   "source": [
    "def evaluate_model(model, test_dataloader):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_dataloader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "        100 * correct / total))"
   ],
   "metadata": {
    "id": "UD3yv1PFSskP",
    "ExecuteTime": {
     "end_time": "2024-04-11T07:13:02.048590Z",
     "start_time": "2024-04-11T07:13:02.036314Z"
    }
   },
   "id": "UD3yv1PFSskP",
   "outputs": [],
   "execution_count": 231
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T07:13:16.317123Z",
     "start_time": "2024-04-11T07:13:10.707979Z"
    }
   },
   "cell_type": "code",
   "source": "evaluate_model(model, test_dataloader)",
   "id": "4a60342d18a79464",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 87 %\n"
     ]
    }
   ],
   "execution_count": 232
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T07:13:20.959453Z",
     "start_time": "2024-04-11T07:13:20.929227Z"
    }
   },
   "cell_type": "code",
   "source": "torch.save(model.state_dict(), 'model.pt')",
   "id": "b631d74f766889fe",
   "outputs": [],
   "execution_count": 233
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Kaggle submission",
   "id": "9ea1737d1a17338b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T07:13:28.170542Z",
     "start_time": "2024-04-11T07:13:28.152536Z"
    }
   },
   "cell_type": "code",
   "source": [
    "transform_nolabel_data = transforms.Compose([ transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)), ])\n"
   ],
   "id": "d42298afec91861a",
   "outputs": [],
   "execution_count": 234
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T07:13:35.629236Z",
     "start_time": "2024-04-11T07:13:35.623237Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import csv\n",
    "\n",
    "def generate_submission_csv(model, cifar_nolabels):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        images = cifar_nolabels[b'data'].reshape((-1, 3, 32, 32)) \n",
    "        images = np.transpose(images, (0,2,3,1))\n",
    "        img_tensor = torch.stack([transform_nolabel_data(img) for img in images]).float().to(device)\n",
    "        outputs = model(img_tensor)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "    \n",
    "    predTuple = list(zip(cifar_nolabels[b'ids'],predicted.cpu().numpy()))\n",
    "\n",
    "    with open('predictions.csv','w',newline='') as out:\n",
    "        csv_out=csv.writer(out)\n",
    "        csv_out.writerow(['ID','Labels'])\n",
    "        csv_out.writerows(predTuple)\n",
    "        "
   ],
   "id": "1ceaf12002482624",
   "outputs": [],
   "execution_count": 235
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T07:13:41.823161Z",
     "start_time": "2024-04-11T07:13:40.507615Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "def load_cifar_batch(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "cifar_nolabels = load_cifar_batch('./data/cifar_test_nolabels.pkl')\n",
    "generate_submission_csv(model, cifar_nolabels)"
   ],
   "id": "493a365149f6d729",
   "outputs": [],
   "execution_count": 236
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Plot the Accuracy of this Model"
   ],
   "metadata": {
    "id": "3eJQUl1vStFL"
   },
   "id": "3eJQUl1vStFL"
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "U81T3Wl9SwJd"
   },
   "id": "U81T3Wl9SwJd",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 8088488,
     "sourceId": 73233,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30673,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 8.271458,
   "end_time": "2024-03-30T01:00:39.910637",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-03-30T01:00:31.639179",
   "version": "2.5.0"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
